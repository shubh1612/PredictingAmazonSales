# -*- coding: utf-8 -*-
"""Predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HSlvr3aJjo8-7zmEctzozoucfbmiCqsx
"""

from google.colab import drive
drive.mount('/content/drive')

!pip3 install cython
!pip3 install tables
!pip3 install statsmodels
!pip3 install -q keras
!pip3 install gensim
!pip3 install nltk

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import svm
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing

import tensorflow as tf

from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Model, load_model
from keras.layers import Input, CuDNNLSTM, Activation, Lambda, Dense, LeakyReLU, Conv1D,GlobalAveragePooling1D
from keras.layers import Dropout, Bidirectional,Concatenate, BatchNormalization, Flatten
from keras import backend as K, regularizers
from keras import optimizers, regularizers, initializers
from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint
from keras.constraints import maxnorm
from keras.regularizers import l2

ratio = 6
data_dir = 'drive/My Drive/'
data_file = 'drive/My Drive/5minNo_normalized_data.h5'

num_prev_values = 12
start = [30]
num_classes = 10
cutoff = []
# data_dir = 'drive/My Drive/btp/sem2_start/Prediction/'
# data_file = 'drive/My Drive/btp/sem2_start/Prediction/No_normalized_data.h5'

df = pd.read_hdf(data_file)

df.head()

df.columns
df.shape

def get_per_claim(df):

	claim = {}

	for index, row in df.iterrows():

		deal_id = row['deal_id']

		if(deal_id in claim):
			claim[deal_id].append(row[0])
		else:
			claim[deal_id] = [row[0]]

	return claim

def x_value(df, num_prev_values, start, claim):

	x = [[]]
	y = []
	x = df.iloc[:, [0, 1, 2, 3, 4, 5, 6, 9\
	 	# 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30 \
	 	]].values

	y = df.iloc[:, [0]].values
	
	actual_x = [[]]
	actual_y = []

	for i in range(len(x)):

		perClaim = []
		asin = (df.iloc[[i]]['deal_id'].values)[0]
		perClaim = claim.get(asin)
		idx = perClaim.index(x[i][0])
		
		for j in range(len(start)):
		
			l = []
			if((idx - start[j] - num_prev_values + 1) < 0):
				continue

			l = (perClaim[(idx-start[j]-num_prev_values+1):(idx-start[j]+1)])

			for k in range(len(x[i]) - 1):
				l.append(x[i][k])

			l.append(start[j])
			actual_x.append(l)
			actual_y.append(y[i])

	actual_y = np.asarray(actual_y)
	
	return actual_x, actual_y

def y_value(y, num_classes, cutoff):

  if(len(cutoff) == 0):
    
    cutoff_pt = 100/num_classes
    num = int(num_classes/cutoff_pt)
    for i in range(0, num_classes):
      y[(y >= i*cutoff_pt) & (y < (i+1)*cutoff_pt)] = i    
    y[y == 100] = num_classes - 1

  else:
    
    y[y <= cutoff[0]] = 0
    for i in range(1, num_classes):
      y[(y > cutoff[i-1]) & (y <= cutoff[i])] = i

  return y

def equalSplit(x, y, ratio, num_classes):

  leny = []
  mini = np.count_nonzero(y==0)

  for i in range(num_classes):
    val = np.count_nonzero(y==i)
    leny.append(val)
    if(val != 0 and mini > val):
      mini = val

  y0 = y[np.where(y == 0), :][0]
  x0 = x[np.where(y == 0), :][0]
  index = np.random.choice(y0.shape[0], min(ratio*mini, leny[0]), replace = False)
  x0 = x0[index]
  y0 = y0[index]

  y1 = y[np.where(y == 1), :][0]
  x1 = x[np.where(y == 1), :][0]
  index = np.random.choice(y1.shape[0], min(ratio*mini, leny[1]), replace = False)
  x1 = x1[index]
  y1 = y1[index]

  actual_x = np.vstack((x0, x1))
  actual_y = np.vstack((y0, y1))

  for i in range(2, num_classes):

    y_prime = []
    x_prime = []
    y_prime = y[np.where(y == i), :][0]
    x_prime = x[np.where(y == i), :][0]

    if(leny[i] == 0):
      print(i)
      continue

    index = np.random.choice(y_prime.shape[0], min(ratio*mini, leny[i]), replace = False)

    x_prime = x_prime[index]
    y_prime = y_prime[index]
    actual_x = np.vstack((actual_x, x_prime))
    actual_y = np.vstack((actual_y, y_prime))

  return x, y

def StandardModel(numFeatures, Loss, optimizer):

	model = Sequential()
	model.add(Dense(12, input_dim = numFeatures, activation = 'relu'))
	model.add(Dense(8, activation = 'relu'))
	model.add(Dense(5, activation = 'softmax'))
	model.compile(loss = Loss, optimizer = optimizer, metrics = ['accuracy'])
	# model.fit(X, Y, epochs=150, batch_size=10,  verbose=2)
	# predictions = model.predict(X)

def define_model(num_classes):

	input = Input(shape = INPUT_SHAPE_1, dtype = 'float32', name = 'features')

	x = Dense(200, kernel_regularizer = regularizers.l2(1e-3) ,name = 'Fully_Connected_Layer_1')(input)
	x = BatchNormalization()(x)
	x = LeakyReLU(alpha=0.01)(x)
	x = Dropout(DROPOUT,  name = 'Dropout_Regularization_1')(x)

	x = Dense(150, kernel_regularizer = regularizers.l2(1e-3), name = 'Fully_Connected_Layer_2')(x)
	x = BatchNormalization()(x)
	x = LeakyReLU(alpha=0.01)(x)
	x = Dropout(DROPOUT,  name = 'Dropout_Regularization_2')(x)

	x = Dense(90, kernel_regularizer = regularizers.l2(1e-3), name = 'Fully_Connected_Layer_3')(x)
	x = BatchNormalization()(x)
	x = LeakyReLU(alpha=0.01)(x)
	x = Dropout(DROPOUT,  name = 'Dropout_Regularization_3')(x)

	x = Dense(50, kernel_regularizer = regularizers.l2(1e-3), name = 'Fully_Connected_Layer_4')(x)
	x = BatchNormalization()(x)
	x = LeakyReLU(alpha=0.01)(x)
	x = Dropout(DROPOUT,  name = 'Dropout_Regularization_4')(x)

	output = Dense(num_classes, activation = 'softmax', name = 'Fully_Connected_Layer_5')(x)

	model = Model(inputs = [input], outputs = [output], name = 'my_model')

	return model

def define_model_2(num_classes):

  input = Input(shape = INPUT_SHAPE_1, dtype = 'float32', name = 'features')

  x = Dense(1024 ,name = 'Fully_Connected_Layer_0')(input)
  # 	x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.01)(x)


  x = Dense(512 ,name = 'Fully_Connected_Layer_1')(x)
  # 	x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.01)(x)
  # 	x = Dropout(DROPOUT,  name = 'Dropout_Regularization_1')(x)

  x = Dense(256, name = 'Fully_Connected_Layer_2')(x)
  # 	x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.01)(x)
  # 	x = Dropout(DROPOUT,  name = 'Dropout_Regularization_2')(x)

  x = Dense(128, name = 'Fully_Connected_Layer_3')(x)
  # 	x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.01)(x)
  # 	x = Dropout(DROPOUT,  name = 'Dropout_Regularization_3')(x)

  x = Dense(64, name = 'Fully_Connected_Layer_4')(x)
  # 	x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.01)(x)
  # 	x = Dropout(DROPOUT,  name = 'Dropout_Regularization_4')(x)

  output = Dense(num_classes, activation = 'softmax', name = 'Fully_Connected_Layer_5')(x)

  model = Model(inputs = [input], outputs = [output], name = 'my_model')

  return model

def run_model(X_train, X_val, Y_train, Y_val, model):

	print(model.summary())
	model.compile(loss='categorical_crossentropy', optimizer = optimizers.Adam(), metrics=['accuracy'])
	earlystop = EarlyStopping(monitor = 'val_loss', patience = PAT)
	check_pt = ModelCheckpoint(data_dir + 'model_.h5', save_best_only=True)
	callbacks_list = [earlystop, check_pt]

	trained_model = model.fit(X_train, [Y_train], epochs = EPOCHS, batch_size = BATCH_SIZE, shuffle = True, \
								validation_data = [X_val, [Y_val]], callbacks = callbacks_list)

	return trained_model, model

def run_model_2(X_train, X_val, Y_train, Y_val, model):

	print(model.summary())
	model.compile(loss='categorical_crossentropy', optimizer = optimizers.Adam(), metrics=['accuracy'])
# 	earlystop = EarlyStopping(monitor = 'val_loss', patience = PAT)
# 	check_pt = ModelCheckpoint(data_dir + 'model_.h5', save_best_only=True)
# 	callbacks_list = [earlystop, check_pt]

	trained_model = model.fit(X_train, [Y_train], epochs = EPOCHS, batch_size = BATCH_SIZE, shuffle = True, \
								validation_data = [X_val, [Y_val]])

	return trained_model, model

def model_history(trained_model):

  fig, axs = plt.subplots(1,2,figsize=(15,5))

  axs[0].plot(trained_model.history['loss'])
  axs[0].plot(trained_model.history['val_loss'])
  axs[0].set_title('Model Loss')
  axs[0].set_ylabel('Loss')
  axs[0].set_xlabel('Epoch')
  axs[0].legend(['Train', 'Validation'], loc='upper right')

  axs[1].plot(trained_model.history['acc'])
  axs[1].plot(trained_model.history['val_acc'])
  axs[1].set_title('Model Accuracy')
  axs[1].set_ylabel('Accuracy')
  axs[1].set_xlabel('Epoch')
  axs[1].legend(['Train', 'Validation'], loc='upper right')
  plt.show()

claim = get_per_claim(df)
x, y = x_value(df, num_prev_values, start, claim)
store_y = y
x = x[1:]
x = np.asarray(x)
dfx = pd.DataFrame(x)
dfx.to_hdf(data_dir + '12_30_x_values.h5', key = 'dfx', mode = 'w')

print(len(x), len(y))
y = y_value(y, num_classes, cutoff)
dfy = pd.DataFrame(y)
dfy.to_hdf(data_dir + '12_30_y.h5', key = 'dfy', mode = 'w')
print('x and y created')
x, y = equalSplit(x, y, ratio, num_classes)
print('x and y equally splitted')

X_scaled = preprocessing.scale(x)
y = y.astype(int)
x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.4, random_state=42)
x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.2, random_state=42)

# print(np.count_nonzero(y_train == 4))
# print(np.count_nonzero(y_train == 3))
# print(np.count_nonzero(y_train == 2))
# print(np.count_nonzero(y_train == 1))
# print(np.count_nonzero(y_train == 0))

y_train = np.squeeze(np.eye(num_classes)[y_train.reshape(-1)])
print(len(X_scaled), len(y))
y_val = np.squeeze(np.eye(num_classes)[y_val.reshape(-1)])
y_test = np.squeeze(np.eye(num_classes)[y_test.reshape(-1)])

# print('Training started')
# clf = MLPClassifier()
# clf.fit(x_train, y_train)

# print('Testing Started')
# y_pred = clf.predict(x_train)
# print('Train Accuracy - ', accuracy_score(y_train, y_pred))
# y_pred = clf.predict(x_test)
# print('Test Accuracy - ', accuracy_score(y_test, y_pred))
# print(len(x_train), len(x_test))

INPUT_SHAPE_1 = (8 + num_prev_values, )
EPOCHS = 1
BATCH_SIZE = 512
DROPOUT = 0.15
PAT = 10

K.clear_session()
model = define_model_2(num_classes)
trained_model, model = run_model_2(x_train, x_val, y_train, y_val, model)
model_history(trained_model)

y_pred = (model.predict(x_test)).argmax(axis=-1)
y_test = [np.where(r==1)[0][0] for r in y_test]
print(accuracy_score(y_test, y_pred))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

y_pred = (model.predict(x_train)).argmax(axis=-1)
y_train = [np.where(r==1)[0][0] for r in y_train]
print(accuracy_score(y_pred, y_train))

from sklearn.metrics import classification_report
print(classification_report(y_train, y_pred))

y_pred = (model.predict(x_val)).argmax(axis=-1)
y_val = [np.where(r==1)[0][0] for r in y_val]
print(accuracy_score(y_val, y_pred))

from sklearn.metrics import classification_report
print(classification_report(y_val, y_pred))

unique, counts = np.unique(y_train, return_counts=True)

print(unique, counts)